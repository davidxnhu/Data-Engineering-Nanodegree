{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pyspark --driver-memory 2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Xiaonan\\\\Courses\\\\Data engineering\\\\Spark\\\\spark-2.2.0-bin-hadoop2.7\\\\bin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.get(\"SPARK_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Xiaonan\\\\Courses\\\\Data engineering\\\\Spark\\\\hadoop-2.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"HADOOP_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getdefaultencoding()\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Create spark session.\n",
    "    Returns:\n",
    "        spark (SparkSession) - spark session connected to AWS EMR cluster\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    '''\n",
    "    Load song data from S3 and extracts the song and artist tables\n",
    "    \n",
    "    Keyword arguments:\n",
    "    spark -- the spark session\n",
    "    input_data -- the path of input song data from S3\n",
    "    output_data -- the path of output data to S3\n",
    "    \n",
    "    '''\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/*json\"\n",
    "    \n",
    "    #song_data = \"./data/song_data/*/*/*/*json\"\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data).drop_duplicates()\n",
    "    df.printSchema()\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table=songs_table.repartition(col(\"year\"),col(\"artist_id\"))\n",
    "    songs_table.write.parquet(output_data + \"songs_table.parquet\", mode=\"overwrite\", partitionBy=[\"year\", \"artist_id\"])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(output_data + \"artists_table.parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    '''\n",
    "    Load log data from S3 and extracts the user, time and songplay tables\n",
    "    \n",
    "    Keyword arguments:\n",
    "    spark -- the spark session\n",
    "    input_data -- the path of input log data from S3\n",
    "    output_data -- the path of output data to S3\n",
    "    \n",
    "    '''\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + \"log_data/*/*/*json\"\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data).drop_duplicates() \n",
    "    df.printSchema()\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page==\"NextSong\")\n",
    "\n",
    "    # extract columns for users table    \n",
    "    #user_table = df.select(\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\").distinct()\n",
    "    \n",
    "    #user_table.show(1)\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    # user_table.write.parquet(output_data + \"user_table.parquet\", mode = \"overwrite\")\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    #get_timestamp = udf(lambda ts: datetime.datetime.fromtimestamp(int(ts)/1000), T.TimestampType())\n",
    "    #df = df.withColumn(\"start_time\", get_timestamp(ts)) \n",
    "    \n",
    "    # extract columns to create time table\n",
    "    #time_table = df.withColumn(\"hour\",     F.hour(\"start_time\"))\\\n",
    "    #                .withColumn(\"day\",     F.dayofmonth(\"start_time\"))\\\n",
    "    #                .withColumn(\"week\",    F.weekofyear(\"start_time\"))\\\n",
    "    #                .withColumn(\"month\",   F.month(\"start_time\"))\\\n",
    "    #                .withColumn(\"year\",    F.year(\"start_time\"))\\\n",
    "    #                .withColumn(\"weekday\", F.dayofweek(\"start_time\"))\\\n",
    "    #                .select(\"ts\",\"start_time\",\"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    #time_table=time_table.repartition(col(\"year\"),col(\"month\"))\n",
    "    #time_table.write.parquet(output_data + \"time_table.parquet\", mode=\"overwrite\", partitionBy = [\"year\", \"month\"])\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    #song_df = spark.read.parquet(output_data + \"songs_table.parquet\")\n",
    "\n",
    "    # read in artist data to use for songplays table\n",
    "    #artist_df = spark.read.parquet(output_data + \"artist_table.parquet\")\n",
    "\n",
    "    # extract columns from joined song, artist and log datasets to create songplays table \n",
    "    #songplays_table = df.selectColumn(\"songplay_id\", F.monotonically_increasing_id())\\\n",
    "    #                    .join(song_df, df.song == song_df.title, how=\"inner\")\\\n",
    "    #                    .select(\"songplay_id\", \n",
    "    #                           \"start_time\", \n",
    "    #                           col(\"userId\").alias(\"user_id\"),\n",
    "    #                            \"level\", \n",
    "    #                            \"song_id\", \n",
    "    #                            \"artist_id\", \n",
    "    #                            col(\"sessionId\").alias(\"session_id\"), \n",
    "    #                            \"location\", \n",
    "    #                            col(\"userAgent\").alias(\"user_agent\")) \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    # songplays_table=songplays_table.repartition(col(\"year\"),col(\"month\"))\n",
    "    # songplays_table.write.parquet(output_data + \"songplays_table.parquet\", mode = \"overwrite\", partitionBy = [\"year\", \"month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4cb95069e9d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.driver.memory'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://udacity-xn/DataEngineering/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#process_song_data(spark, input_data, output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#song_data = \"./data/song_data/*/*/*/*json\"\n",
    "    \n",
    "# read song data file\n",
    "#df = spark.read.json(song_data).drop_duplicates()\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sc=spark.sparkContext\n",
    "#hadoop_conf=sc._jsc.hadoopConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#hadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "#hadoop_conf.set(\"fs.s3n.awsAccessKeyId\", os.environ[\"AWS_ACCESS_KEY_ID\"])\n",
    "#hadoop_conf.set(\"fs.s3n.awsSecretAccessKey\", os.environ[\"AWS_SECRET_ACCESS_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#songs_table.write.parquet(output_data + \"songs_table.parquet\", mode=\"overwrite\", partitionBy=[\"year\", \"artist_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + \"log_data/*/*/*json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read log data file\n",
    "df = spark.read.json(log_data).drop_duplicates() \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page==\"NextSong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda ts: datetime.fromtimestamp(int(ts)/1000), T.TimestampType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(\"ts\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df.withColumn(\"hour\",     F.hour(\"start_time\"))\\\n",
    "                .withColumn(\"day\",     F.dayofmonth(\"start_time\"))\\\n",
    "                .withColumn(\"week\",    F.weekofyear(\"start_time\"))\\\n",
    "                .withColumn(\"month\",   F.month(\"start_time\"))\\\n",
    "                .withColumn(\"year\",    F.year(\"start_time\"))\\\n",
    "                .withColumn(\"weekday\", F.dayofweek(\"start_time\"))\\\n",
    "                .select(\"ts\",\"start_time\",\"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>start_time</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1542296032796</td>\n",
       "      <td>2018-11-15 16:33:52.796</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ts              start_time  hour  day  week  month  year  \\\n",
       "0  1542296032796 2018-11-15 16:33:52.796    16   15    46     11  2018   \n",
       "\n",
       "   weekday  \n",
       "0        5  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table=time_table.repartition(col(\"year\"),col(\"month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.write.parquet(output_data + \"time_table.parquet\", mode=\"overwrite\", partitionBy = [\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.parquet(output_data + \"songs_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fat Joe</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kate</td>\n",
       "      <td>F</td>\n",
       "      <td>21</td>\n",
       "      <td>Harrell</td>\n",
       "      <td>241.34485</td>\n",
       "      <td>paid</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540473e+12</td>\n",
       "      <td>605</td>\n",
       "      <td>Safe 2 Say [The Incredible] (Album Version - A...</td>\n",
       "      <td>200</td>\n",
       "      <td>1542296032796</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>97</td>\n",
       "      <td>2018-11-15 16:33:52.796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist       auth firstName gender  itemInSession lastName     length  \\\n",
       "0  Fat Joe  Logged In      Kate      F             21  Harrell  241.34485   \n",
       "\n",
       "  level                  location method      page  registration  sessionId  \\\n",
       "0  paid  Lansing-East Lansing, MI    PUT  NextSong  1.540473e+12        605   \n",
       "\n",
       "                                                song  status             ts  \\\n",
       "0  Safe 2 Say [The Incredible] (Album Version - A...     200  1542296032796   \n",
       "\n",
       "                                           userAgent userId  \\\n",
       "0  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...     97   \n",
       "\n",
       "               start_time  \n",
       "0 2018-11-15 16:33:52.796  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_df = spark.read.parquet(output_data + \"artists_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "#songplays_table = df.withColumn(\"songplay_id\", F.monotonically_increasing_id())\\\n",
    "#               .join(song_df, df.song == song_df.title, how=\"left_outer\")\\\n",
    "#               .join(artist_df, df.artist == artist_df.artist_name, how=\"left_outer\")\\\n",
    "#               .select(\"songplay_id\", \n",
    "#                      \"start_time\", \n",
    "#                       col(\"userId\").alias(\"user_id\"),\n",
    "#                        \"level\", \n",
    "#                        \"song_id\", \n",
    "#                        artist_df[\"artist_id\"].alias(\"artist_id\"), \n",
    "#                        col(\"sessionId\").alias(\"session_id\"), \n",
    "#                        \"location\", \n",
    "#                        col(\"userAgent\").alias(\"user_agent\"),\n",
    "#                        F.year(\"start_time\").alias(\"year\"),\n",
    "#                        F.month(\"start_time\").alias(\"month\")\n",
    "\n",
    "#                      ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " df_joined_songs_artists = song_df.join(artist_df, 'artist_id').select(\"artist_id\", \"song_id\",\n",
    "                                                                                      \"title\", \"artist_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table = df.withColumn(\"songplay_id\", F.monotonically_increasing_id())\\\n",
    "               .join(df_joined_songs_artists, df.artist == df_joined_songs_artists.artist_name)\\\n",
    "               .select(\"songplay_id\", \n",
    "                      \"start_time\", \n",
    "                       col(\"userId\").alias(\"user_id\"),\n",
    "                        \"level\", \n",
    "                        \"song_id\", \n",
    "                        \"artist_id\", \n",
    "                        col(\"sessionId\").alias(\"session_id\"), \n",
    "                        \"location\", \n",
    "                        col(\"userAgent\").alias(\"user_agent\"),\n",
    "                        F.year(\"start_time\").alias(\"year\"),\n",
    "                        F.month(\"start_time\").alias(\"month\")\n",
    "\n",
    "                      ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- songplay_id: long (nullable = false)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table=songplays_table.repartition(col(\"year\"),col(\"month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.write.parquet(output_data + \"songplays_table.parquet\", mode = \"overwrite\", partitionBy = [\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.19.11-py3-none-any.whl (131 kB)\n",
      "Collecting botocore<1.23.0,>=1.22.11\n",
      "  Downloading botocore-1.22.11-py3-none-any.whl (8.1 MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\xiaonan\\anaconda3\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\xiaonan\\anaconda3\\lib\\site-packages (from botocore<1.23.0,>=1.22.11->boto3) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\xiaonan\\anaconda3\\lib\\site-packages (from botocore<1.23.0,>=1.22.11->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\xiaonan\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.11->boto3) (1.15.0)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.18.16\n",
      "    Uninstalling botocore-1.18.16:\n",
      "      Successfully uninstalled botocore-1.18.16\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.3.3\n",
      "    Uninstalling s3transfer-0.3.3:\n",
      "      Successfully uninstalled s3transfer-0.3.3\n",
      "Successfully installed boto3-1.19.11 botocore-1.22.11 s3transfer-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: awscli 1.18.157 has requirement botocore==1.18.16, but you'll have botocore 1.22.11 which is incompatible.\n",
      "ERROR: awscli 1.18.157 has requirement s3transfer<0.4.0,>=0.3.0, but you'll have s3transfer 0.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-east-1\",\n",
    "                       aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-01-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-02-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-03-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-04-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-05-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-06-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-07-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-08-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-09-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-10-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-11-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-12-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-13-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-14-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-15-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-16-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-17-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-18-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-19-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-20-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-21-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-22-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-23-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-24-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-25-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-26-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-27-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-28-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-29-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-30-events.json')\n"
     ]
    }
   ],
   "source": [
    "sampleDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "# TODO: Iterate over bucket objects starting with \"ssbgz\" and print\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"log_data\"):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songplay_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>level</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>location</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1056561954818</td>\n",
       "      <td>2018-11-21 09:25:43.796</td>\n",
       "      <td>88</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOFNUGC12A6BD530ED</td>\n",
       "      <td>ARN8NCB1187FB49652</td>\n",
       "      <td>744</td>\n",
       "      <td>Sacramento--Roseville--Arden-Arcade, CA</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     songplay_id              start_time user_id level             song_id  \\\n",
       "0  1056561954818 2018-11-21 09:25:43.796      88  paid  SOFNUGC12A6BD530ED   \n",
       "\n",
       "            artist_id  session_id                                 location  \\\n",
       "0  ARN8NCB1187FB49652         744  Sacramento--Roseville--Arden-Arcade, CA   \n",
       "\n",
       "                                          user_agent  year  month  \n",
       "0  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...  2018     11  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplays_table.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
