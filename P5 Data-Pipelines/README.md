## Introduction

A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

They have decided to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

## Project Description

This project will introduce us to the core concepts of Apache Airflow. To complete the project, we will need to create our own custom operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.


In addition, configure the task dependencies so that after the dependencies are set, the graph view follows the flow shown in the image below.

![image1](image/example-dag.png)

# Project Datasets

We'll be working with two datasets that reside in S3. Here are the S3 links for each:

* Song data: 
    s3://udacity-dend/song_data
    
* Log data: 
    s3://udacity-dend/log_data
    
## Song dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.  Here is an example of song file:

    {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}


## Log Dataset
The second dataset consists of log files in JSON format generated by event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset we'll be working with are partitioned by year and month. Here is an example of log file:

![image2](image/log-data.png)

## Repository files

```
airflow
|
|____dags
| |____ create_tables_dag.py   # DAG for creating tables on Amazon Redshift
| |____ create_tables.sql      # SQL CREATE queries to be consumed by create_tables_dag.py
| |____ udac_example_dag.py    # Main DAG for the data pipeline including the staging from S3 to redshift, load fact and dimension tables
|
|____plugins
| |____ __init__.py
| |
| |____operators
| | |____ __init__.py          # Define operators and helpers
| | |____ stage_redshift.py    # COPY data from S3 to Redshift
| | |____ load_fact.py         # Execute INSERT query into fact table
| | |____ load_dimension.py    # Execute INSERT queries into dimension tables
| | |____ data_quality.py      # Data quality check after pipeline execution
| |
| |____helpers
| | |____ __init__.py
| | |____ sql_queries.py       # SQL queries for building up dimensional tables

image                          # images for READMD file

README.md                      # README file

```


## How to run

1. Create a Redshift cluster on your AWS account

2. Start the airflow UI

``` 
/opt/airflow/start.sh
```

3. Create AWS and Redshift connections on Airflow Web UI

4. Create the tables in Amazon redshift, and run the main data pipeline by clicking the corresponding DAG in airflow UI.

### Operators

1. `Begin_execution` & `End_execution`

    Dummy operators at the beginning and the end of the data pipeline

2. `Stage operator`

   The stage operator is expected to be able to load any JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. 

3. `Fact and Dimension Operators`
    
    With dimension and fact operators, you can utilize the provided SQL helper class to run data transformations. 

4. `Data Quality Operator` 
    
    Check no empty table after data loading. Check that certain columns does not contain NULL data (e.g. id). More tests could be customized and added.
